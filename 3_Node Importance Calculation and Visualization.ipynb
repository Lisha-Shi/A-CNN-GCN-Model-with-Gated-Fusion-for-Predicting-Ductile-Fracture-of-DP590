{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def96c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_importance_by_removal(model, data, layer_name=\"Layer1\", criterion=nn.MSELoss()):\n",
    "    \"\"\"\n",
    "    Calculate real node importance by node removal (layer-wise calculation)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get original node features\n",
    "        x_all = data.x\n",
    "        num_nodes = data.num_nodes\n",
    "        \n",
    "        # Run CNN and fusion layers (fixed part)\n",
    "        x_cnn = x_all[:, :-1].view(num_nodes, 1, 21, 21)\n",
    "        x_mfrac = x_all[:, -1].unsqueeze(1)\n",
    "        \n",
    "        x_cnn = model.cnn(x_cnn)\n",
    "        x_cnn = x_cnn.view(num_nodes, -1)\n",
    "        x, gate_weights = model.fusion(x_cnn, x_mfrac)\n",
    "        \n",
    "        # Key fix: Layer-wise calculation\n",
    "        if layer_name == \"Layer1\":\n",
    "            # Layer 1: Use fused features\n",
    "            layer_output = F.relu(model.conv1(x, data.edge_index))\n",
    "            original_loss = criterion(layer_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "        else:\n",
    "            # Layer 2: Use Layer 1 output as input\n",
    "            layer1_output = F.relu(model.conv1(x, data.edge_index))\n",
    "            layer_output = F.relu(model.conv2(layer1_output, data.edge_index))\n",
    "            original_loss = criterion(layer_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "        \n",
    "        node_importance = []\n",
    "        removal_effects = []\n",
    "        \n",
    "        print(f\"Analyzing {num_nodes} node importance for {layer_name}...\")\n",
    "        \n",
    "        for node_idx in range(num_nodes):\n",
    "            # Create node removal mask (set this node's features to zero)\n",
    "            modified_x = x.clone()\n",
    "            modified_x[node_idx] = 0  # Key operation: remove node features\n",
    "            \n",
    "            # Also remove all edges connected to this node\n",
    "            edge_mask = (data.edge_index[0] != node_idx) & (data.edge_index[1] != node_idx)\n",
    "            modified_edge_index = data.edge_index[:, edge_mask]\n",
    "            \n",
    "            if layer_name == \"Layer1\":\n",
    "                # Layer 1: Use modified features and edges\n",
    "                modified_output = F.relu(model.conv1(modified_x, modified_edge_index))\n",
    "                modified_loss = criterion(modified_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "            else:\n",
    "                # Layer 2: Use Layer 1 output\n",
    "                layer1_output_modified = F.relu(model.conv1(modified_x, modified_edge_index))\n",
    "                modified_output = F.relu(model.conv2(layer1_output_modified, modified_edge_index))\n",
    "                modified_loss = criterion(modified_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "            \n",
    "            # Calculate importance metrics\n",
    "            importance = abs(modified_loss - original_loss)\n",
    "            effect = modified_loss - original_loss  # Positive effect indicates important node\n",
    "            \n",
    "            node_importance.append(importance)\n",
    "            removal_effects.append(effect)\n",
    "            \n",
    "            if (node_idx + 1) % 5 == 0 or (node_idx + 1) == num_nodes:\n",
    "                print(f\"Analyzed {node_idx + 1}/{num_nodes} nodes\")\n",
    "        \n",
    "        # Normalize importance scores to [0,1] range\n",
    "        importance_scores = np.array(node_importance)\n",
    "        if importance_scores.max() > importance_scores.min():\n",
    "            importance_scores = (importance_scores - importance_scores.min()) / (importance_scores.max() - importance_scores.min())\n",
    "        else:\n",
    "            importance_scores = np.ones_like(importance_scores) * 0.5\n",
    "        \n",
    "        return {\n",
    "            'importance_scores': importance_scores,\n",
    "            'removal_effects': np.array(removal_effects),\n",
    "            'original_loss': original_loss,\n",
    "            'node_features': data.x.cpu().numpy(),\n",
    "            'edge_index': data.edge_index.cpu().numpy(),\n",
    "            'layer_name': layer_name\n",
    "        }\n",
    "\n",
    "def visualize_node_importance(model, data, importance_results, title=\"Node Importance by Removal\",\n",
    "                            inner_label_size=10, outer_label_size=8, label_offset=0.15,\n",
    "                            min_node_size=800, max_node_size=2800):\n",
    "    \"\"\"\n",
    "    Visualize node importance (enhanced version)\n",
    "    Parameters:\n",
    "        inner_label_size: Size of circle inner number label\n",
    "        outer_label_size: Size of circle outer importance label\n",
    "        label_offset: Offset for outer label\n",
    "        min_node_size: Minimum node size (default 800)\n",
    "        max_node_size: Maximum node size (default 2800)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    importance_scores = importance_results['importance_scores']\n",
    "    layer_name = importance_results['layer_name']\n",
    "    \n",
    "    # Create network graph\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    pos = nx.shell_layout(G)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Calculate node sizes (linear interpolation based on importance scores)\n",
    "    node_sizes = min_node_size + (max_node_size - min_node_size) * importance_scores\n",
    "    \n",
    "    # Draw nodes (color represents importance)\n",
    "    nodes = nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        node_size=node_sizes,\n",
    "        node_color=importance_scores,\n",
    "        cmap='cividis',\n",
    "        vmin=0, vmax=1,\n",
    "        alpha=0.9,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Add colorbar\n",
    "#     sm = plt.cm.ScalarMappable(cmap=plt.cm.cividis, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "#     sm.set_array([])\n",
    "#     cbar = plt.colorbar(sm, ax=ax, label='Node Importance')\n",
    "#     cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    \n",
    "    \n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.cividis, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, label='Node Importance')\n",
    "    cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "    # Set tick label font size\n",
    "    cbar.ax.tick_params(labelsize=35)  # Number label font size\n",
    "\n",
    "    # Set colorbar title font size\n",
    "    cbar.set_label('Node Importance', size=35, weight='normal')  # Title font size\n",
    "    \n",
    "    \n",
    "    # Draw edges (gray translucent)\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        width=1.5,\n",
    "        edge_color='gray',\n",
    "        alpha=0.5,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Add node labels (inner and outer layers)\n",
    "    for node_idx, (x, y) in pos.items():\n",
    "        # Inner label (node number)\n",
    "        ax.text(x, y, str(node_idx), \n",
    "               fontsize=inner_label_size, \n",
    "               fontweight='bold',\n",
    "               ha='center', va='center',\n",
    "               color='black' if importance_scores[node_idx] > 0.5 else 'white')\n",
    "        \n",
    "        # Outer label (importance score)\n",
    "        if importance_scores[node_idx] >= 0:\n",
    "            # Calculate label position (offset along radius)\n",
    "            radius = np.sqrt(node_sizes[node_idx]) / 100\n",
    "            angle = np.arctan2(y, x) if x != 0 else np.pi/2\n",
    "            offset_x = label_offset * np.cos(angle)\n",
    "            offset_y = label_offset * np.sin(angle)\n",
    "            \n",
    "            ax.text(x + offset_x, y + offset_y, \n",
    "                   f\"{importance_scores[node_idx]:.2f}\",\n",
    "                   fontsize=outer_label_size,\n",
    "                   fontweight='normal',\n",
    "                   ha='center', va='center',\n",
    "                   )#bbox=dict(facecolor='white', edgecolor='none', alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f\"{layer_name} - {title}\", fontsize=16, pad=20)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def comprehensive_node_importance_analysis(model, sample_data,\n",
    "                                         inner_label_size=32,\n",
    "                                         outer_label_size=28,\n",
    "                                         label_offset=0.225,\n",
    "                                         min_node_size=2000,\n",
    "                                         max_node_size=5000):\n",
    "    \"\"\"\n",
    "    Comprehensive node importance analysis (complete version)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Real node importance analysis based on node removal (layer-wise)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze Layer 1\n",
    "    print(\"\\n=== GCN Layer 1 Analysis ===\")\n",
    "    layer1_results = calculate_node_importance_by_removal(model, sample_data, \"Layer1\")\n",
    "    visualize_node_importance(\n",
    "        model, sample_data, layer1_results,\n",
    "        inner_label_size=inner_label_size,\n",
    "        outer_label_size=outer_label_size,\n",
    "        label_offset=label_offset,\n",
    "        min_node_size=min_node_size,\n",
    "        max_node_size=max_node_size\n",
    "    )\n",
    "    \n",
    "    # Analyze Layer 2\n",
    "    print(\"\\n=== GCN Layer 2 Analysis ===\")\n",
    "    layer2_results = calculate_node_importance_by_removal(model, sample_data, \"Layer2\")\n",
    "    visualize_node_importance(\n",
    "        model, sample_data, layer2_results,\n",
    "        inner_label_size=inner_label_size,\n",
    "        outer_label_size=outer_label_size,\n",
    "        label_offset=label_offset,\n",
    "        min_node_size=min_node_size,\n",
    "        max_node_size=max_node_size\n",
    "    )\n",
    "    \n",
    "    # Compare results between two layers\n",
    "    print(\"\\n=== Comparison of Node Importance Between Layers ===\")\n",
    "    correlation = np.corrcoef(layer1_results['importance_scores'], \n",
    "                             layer2_results['importance_scores'])[0,1]\n",
    "    differences = np.abs(layer1_results['importance_scores'] - layer2_results['importance_scores'])\n",
    "    \n",
    "    print(f\"Layer 1 average importance: {layer1_results['importance_scores'].mean():.4f}\")\n",
    "    print(f\"Layer 2 average importance: {layer2_results['importance_scores'].mean():.4f}\")\n",
    "    print(f\"Correlation between layers: {correlation:.4f}\")\n",
    "    print(f\"Average difference: {differences.mean():.4f}\")\n",
    "    print(f\"Maximum difference: {differences.max():.4f}\")\n",
    "    \n",
    "    # Find the most important nodes\n",
    "    top_k = 3\n",
    "    for layer_name, results in [(\"Layer 1\", layer1_results), (\"Layer 2\", layer2_results)]:\n",
    "        scores = results['importance_scores']\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        print(f\"\\n{layer_name} most important {top_k} nodes:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            importance = scores[idx]\n",
    "            effect = results['removal_effects'][idx]\n",
    "            print(f\"  {i+1}. Node {idx}: Importance={importance:.4f}, Effect={effect:.6f}\")\n",
    "    \n",
    "    return layer1_results, layer2_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Select sample for analysis\n",
    "    sample_idx = 1  # Can be modified to any sample index\n",
    "    sample_data = train_dataset[sample_idx]\n",
    "    \n",
    "    # Print basic sample information\n",
    "    print(f\"\\nAnalyzing sample {sample_idx} (total {len(train_dataset)} samples)\")\n",
    "    print(f\"Number of nodes: {sample_data.num_nodes}\")\n",
    "    print(f\"Number of edges: {sample_data.edge_index.shape[1]}\")\n",
    "    \n",
    "    # Execute node importance analysis\n",
    "    layer1_results, layer2_results = comprehensive_node_importance_analysis(\n",
    "        model, sample_data,\n",
    "        inner_label_size=32,\n",
    "        outer_label_size=28,\n",
    "        label_offset=0.2,\n",
    "        min_node_size=1200,\n",
    "        max_node_size=3500\n",
    "    )\n",
    "    \n",
    "    # Show microstructure images and DamageStrain values at the end\n",
    "    print(\"\\n=== Displaying microstructure images and DamageStrain values ===\")\n",
    "    visualize_microstructures_with_damage(\n",
    "        sample_data,\n",
    "        cmap='gray_r',\n",
    "        figsize=(20, 4),\n",
    "        border_color='k',\n",
    "        border_width=6\n",
    "    )\n",
    "    \n",
    "    # Print analysis completion message\n",
    "    print(\"\\n=== Analysis Completed ===\")\n",
    "    print(f\"Node importance analysis and microstructure visualization for sample {sample_idx} completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
