{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af082d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average prediction error for each graph (load one graph at a time)\n",
    "\n",
    "def evaluate_per_graph(model, dataset, dataset_name=\"train\"):\n",
    "    \"\"\"Calculate average true value, average prediction, and error for each graph\"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    graph_metrics = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            preds = model(data).cpu().numpy()\n",
    "            trues = data.y.cpu().numpy()\n",
    "\n",
    "            mean_pred = float(np.mean(preds))\n",
    "            mean_true = float(np.mean(trues))\n",
    "            mean_error = abs(mean_pred - mean_true)\n",
    "\n",
    "            graph_metrics.append({\n",
    "                'index': idx,\n",
    "                'mean_pred': mean_pred,\n",
    "                'mean_true': mean_true,\n",
    "                'mean_error': mean_error\n",
    "            })\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} set sample mean error statistics completed, total {len(graph_metrics)} graphs ===\")\n",
    "    return graph_metrics\n",
    "\n",
    "\n",
    "# Use original dataset (not DataLoader)\n",
    "train_graph_metrics = evaluate_per_graph(model, train_dataset, \"Training\")\n",
    "test_graph_metrics = evaluate_per_graph(model, test_dataset, \"Test\")\n",
    "\n",
    "# Sort by error\n",
    "train_sorted = sorted(train_graph_metrics, key=lambda x: x['mean_error'])\n",
    "test_sorted = sorted(test_graph_metrics, key=lambda x: x['mean_error'])\n",
    "\n",
    "# Take top 10 best samples and top 10 worst samples\n",
    "best_train = train_sorted[:10]\n",
    "worst_train = train_sorted[-10:]\n",
    "best_test = test_sorted[:10]\n",
    "worst_test = test_sorted[-10:]\n",
    "\n",
    "# =============== Print results ===============\n",
    "def print_graph_stats(title, items):\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    print(f\"{'Graph Index':>10} {'True Mean':>12} {'Pred Mean':>12} {'Abs Error':>12}\")\n",
    "    for item in items:\n",
    "        print(f\"{item['index']:>10} {item['mean_true']:.6f} {item['mean_pred']:.6f} {item['mean_error']:.6f}\")\n",
    "\n",
    "# Training set results\n",
    "print_graph_stats(\"Top 10 Best Predicted Graph Structures in Training Set (Smallest Error)\", best_train)\n",
    "print_graph_stats(\"Top 10 Worst Predicted Graph Structures in Training Set (Largest Error)\", worst_train)\n",
    "\n",
    "# Test set results\n",
    "print_graph_stats(\"Top 10 Best Predicted Graph Structures in Test Set (Smallest Error)\", best_test)\n",
    "print_graph_stats(\"Top 10 Worst Predicted Graph Structures in Test Set (Largest Error)\", worst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def calculate_real_edge_importance_by_removal(model, data, layer_name=\"Layer1\", criterion=nn.MSELoss()):\n",
    "    \"\"\"Calculate edge importance by removing edges and measuring loss change\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get original node features\n",
    "        x_all = data.x\n",
    "        num_nodes = data.num_nodes\n",
    "        \n",
    "        # Run CNN and fusion layers (fixed part)\n",
    "        x_cnn = x_all[:, :-1].view(num_nodes, 1, 21, 21)\n",
    "        x_mfrac = x_all[:, -1].unsqueeze(1)\n",
    "        \n",
    "        x_cnn = model.cnn(x_cnn)\n",
    "        x_cnn = x_cnn.view(num_nodes, -1)\n",
    "        x, gate_weights = model.fusion(x_cnn, x_mfrac)\n",
    "        \n",
    "        edge_index = data.edge_index\n",
    "        num_edges = edge_index.shape[1]\n",
    "        \n",
    "        # Key fix: Layer-wise calculation\n",
    "        if layer_name == \"Layer1\":\n",
    "            # Layer 1: Use fused features\n",
    "            layer_output = F.relu(model.conv1(x, edge_index))\n",
    "            # Use node-level prediction loss\n",
    "            original_loss = criterion(layer_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "        else:\n",
    "            # Layer 2: Use Layer 1 output as input\n",
    "            layer1_output = F.relu(model.conv1(x, edge_index))\n",
    "            layer_output = F.relu(model.conv2(layer1_output, edge_index))\n",
    "            original_loss = criterion(layer_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "        \n",
    "        edge_importance_scores = []\n",
    "        edge_removal_effects = []\n",
    "        \n",
    "        print(f\"Analyzing {num_edges} edge importance for {layer_name}...\")\n",
    "        \n",
    "        for edge_idx in range(num_edges):\n",
    "            # Create graph structure with single edge removed\n",
    "            mask = torch.ones(num_edges, dtype=torch.bool)\n",
    "            mask[edge_idx] = False\n",
    "            \n",
    "            modified_edge_index = edge_index[:, mask]\n",
    "            \n",
    "            if layer_name == \"Layer1\":\n",
    "                # Layer 1: Use same input x, but modified edge index\n",
    "                modified_output = F.relu(model.conv1(x, modified_edge_index))\n",
    "                modified_loss = criterion(modified_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "            else:\n",
    "                # Layer 2: Use Layer 1 output, but modified edge index\n",
    "                layer1_output_modified = F.relu(model.conv1(x, modified_edge_index))\n",
    "                modified_output = F.relu(model.conv2(layer1_output_modified, modified_edge_index))\n",
    "                modified_loss = criterion(modified_output.mean(dim=1, keepdim=True), data.y).item()\n",
    "            \n",
    "            # Calculate importance: larger loss change means more important edge\n",
    "            importance = abs(modified_loss - original_loss)\n",
    "            effect = modified_loss - original_loss\n",
    "            \n",
    "            edge_importance_scores.append(importance)\n",
    "            edge_removal_effects.append(effect)\n",
    "            \n",
    "            if (edge_idx + 1) % 10 == 0 or (edge_idx + 1) == num_edges:\n",
    "                print(f\"Analyzed {edge_idx + 1}/{num_edges} edges\")\n",
    "        \n",
    "        # Normalize importance scores to [0,1] range\n",
    "        importance_scores = np.array(edge_importance_scores)\n",
    "        if importance_scores.max() > importance_scores.min():\n",
    "            importance_scores = (importance_scores - importance_scores.min()) / (importance_scores.max() - importance_scores.min())\n",
    "        else:\n",
    "            importance_scores = np.ones_like(importance_scores) * 0.5\n",
    "        \n",
    "        return {\n",
    "            'importance_scores': importance_scores,\n",
    "            'removal_effects': np.array(edge_removal_effects),\n",
    "            'original_loss': original_loss,\n",
    "            'edge_index': edge_index.cpu().numpy(),\n",
    "            'layer_name': layer_name\n",
    "        }\n",
    "\n",
    "def visualize_real_gcn_attention(model, sample_data, importance_results, title_suffix=\"\",\n",
    "                               node_size=1600, label_font_size=14, edge_width_multiplier=4,\n",
    "                               edge_label_size=10, show_edge_labels=True, \n",
    "                               edge_label_color='darkred', label_pos_offset=0.3):\n",
    "    \"\"\"\n",
    "    Visualize real GCN edge importance (enhanced version)\n",
    "    Parameters:\n",
    "        edge_label_color: Edge label font color (default 'darkred')\n",
    "        label_pos_offset: Label position offset (default 0.3), prevents label overlapping with edges\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    sample_data = sample_data.to(device)\n",
    "    \n",
    "    importance_scores = importance_results['importance_scores']\n",
    "    layer_name = importance_results['layer_name']\n",
    "    \n",
    "    # Create network graph\n",
    "    G = to_networkx(sample_data, to_undirected=True)\n",
    "    pos = nx.shell_layout(G)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    \n",
    "    # Draw nodes (using real Mfraction values)\n",
    "    node_features = sample_data.x.detach().cpu().numpy()\n",
    "    node_colors = node_features[:, -1]  # Mfraction values\n",
    "    \n",
    "    # Fixed colormap range\n",
    "    vmin, vmax = 0, 0.26\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
    "                                 cmap='viridis', vmin=vmin, vmax=vmax, \n",
    "                                 node_size=node_size, ax=ax)\n",
    "    \n",
    "    # Add Mfraction colorbar\n",
    "    cbar_mfrac = plt.colorbar(nodes, ax=ax, label='Global Martensite Fraction Value')\n",
    "    cbar_mfrac.set_ticks([0, 0.052, 0.104, 0.156, 0.208, 0.26])\n",
    "    \n",
    "    # Draw edges (using real edge importance)\n",
    "    edge_widths = importance_scores * edge_width_multiplier + 1\n",
    "    edge_colors = plt.cm.Blues(importance_scores)\n",
    "    edges = nx.draw_networkx_edges(G, pos, width=edge_widths,\n",
    "                                  edge_color=edge_colors, ax=ax)\n",
    "    \n",
    "    # Add edge labels (with anti-overlap handling)\n",
    "    if show_edge_labels:\n",
    "        edge_labels = {}\n",
    "        label_pos = {}\n",
    "        \n",
    "        # Ensure edge indices correspond to importance scores\n",
    "        edge_list = list(G.edges())\n",
    "        for i, (u, v) in enumerate(edge_list):\n",
    "            if i >= len(importance_scores):\n",
    "                break  # Prevent index out of bounds\n",
    "                \n",
    "            edge_labels[(u, v)] = f\"{importance_scores[i]:.2f}\"\n",
    "            \n",
    "            # Calculate label position (offset along edge)\n",
    "            if u in pos and v in pos:  # Ensure nodes are in position dictionary\n",
    "                x1, y1 = pos[u]\n",
    "                x2, y2 = pos[v]\n",
    "                dx = x2 - x1\n",
    "                dy = y2 - y1\n",
    "                length = np.sqrt(dx**2 + dy**2)\n",
    "                \n",
    "                if length > 0:\n",
    "                    dx /= length\n",
    "                    dy /= length\n",
    "                    \n",
    "                # Apply offset\n",
    "                offset_x = label_pos_offset * dy  # Vertical offset\n",
    "                offset_y = -label_pos_offset * dx # Vertical offset\n",
    "                \n",
    "                label_pos[(u, v)] = ((x1 + x2)/2 + offset_x, \n",
    "                                    (y1 + y2)/2 + offset_y)\n",
    "        \n",
    "        # Draw edge labels\n",
    "        if edge_labels and label_pos:\n",
    "            nx.draw_networkx_edge_labels(\n",
    "                G, \n",
    "                pos=pos,  # Use original layout as base\n",
    "                edge_labels=edge_labels,\n",
    "                label_pos=0.5,  # Default middle position\n",
    "                font_color=edge_label_color,\n",
    "                font_size=edge_label_size,\n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.7),\n",
    "                ax=ax,\n",
    "                rotate=False  # Don't rotate labels\n",
    "            )\n",
    "    \n",
    "    # Add attention weight colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar_attn = plt.colorbar(sm, ax=ax, label='Edge Importance')\n",
    "\n",
    "    # Set tick label font size\n",
    "    cbar_attn.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    cbar_attn.ax.tick_params(labelsize=35)  # Set tick label font size\n",
    "\n",
    "    # Set colorbar title font size\n",
    "    cbar_attn.set_label('Edge Importance', size=35, weight='normal')  # Set title font size and weight\n",
    "    \n",
    "    # Add node labels\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax, font_size=label_font_size, font_weight='bold')\n",
    "    \n",
    "    # Set title\n",
    "    title = f\"GCN {layer_name} Edge Importance\" + title_suffix\n",
    "    ax.set_title(title, fontsize=16, pad=20)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "def analyze_layer_comparison(layer1_results, layer2_results):\n",
    "    \"\"\"\n",
    "    Analyze differences between two GCN layers\n",
    "    \"\"\"\n",
    "    layer1_scores = layer1_results['importance_scores']\n",
    "    layer2_scores = layer2_results['importance_scores']\n",
    "    \n",
    "    correlation = np.corrcoef(layer1_scores, layer2_scores)[0,1]\n",
    "    differences = np.abs(layer1_scores - layer2_scores)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GCN Layer Edge Importance Comparison Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Layer 1 average importance: {layer1_scores.mean():.4f}\")\n",
    "    print(f\"Layer 2 average importance: {layer2_scores.mean():.4f}\")\n",
    "    print(f\"Correlation between layers: {correlation:.4f}\")\n",
    "    print(f\"Average difference: {differences.mean():.4f}\")\n",
    "    print(f\"Maximum difference: {differences.max():.4f}\")\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "def comprehensive_gcn_analysis(model, sample_data, node_size=1600, label_font_size=14, \n",
    "                             edge_width_multiplier=4, edge_label_size=10, show_edge_labels=True,\n",
    "                             edge_label_color='darkred', label_pos_offset=0.3):\n",
    "    \"\"\"\n",
    "    Comprehensive GCN edge importance analysis\n",
    "    Parameters:\n",
    "        edge_label_color: Edge label font color (default 'darkred')\n",
    "        label_pos_offset: Label position offset (default 0.3)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Starting real GCN edge importance analysis based on edge removal\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze Layer 1\n",
    "    print(\"\\n=== GCN Layer 1 Analysis ===\")\n",
    "    layer1_results = calculate_real_edge_importance_by_removal(model, sample_data, \"Layer1\")\n",
    "    layer1_scores = visualize_real_gcn_attention(\n",
    "        model, sample_data, layer1_results, \n",
    "        \" - Real Importance\",\n",
    "        node_size=node_size,\n",
    "        label_font_size=label_font_size,\n",
    "        edge_width_multiplier=edge_width_multiplier,\n",
    "        edge_label_size=edge_label_size,\n",
    "        show_edge_labels=show_edge_labels,\n",
    "        edge_label_color=edge_label_color,\n",
    "        label_pos_offset=label_pos_offset\n",
    "    )\n",
    "    \n",
    "    # Analyze Layer 2\n",
    "    print(\"\\n=== GCN Layer 2 Analysis ===\")\n",
    "    layer2_results = calculate_real_edge_importance_by_removal(model, sample_data, \"Layer2\")\n",
    "    layer2_scores = visualize_real_gcn_attention(\n",
    "        model, sample_data, layer2_results, \n",
    "        \" - Real Importance\",\n",
    "        node_size=node_size,\n",
    "        label_font_size=label_font_size,\n",
    "        edge_width_multiplier=edge_width_multiplier,\n",
    "        edge_label_size=edge_label_size,\n",
    "        show_edge_labels=show_edge_labels,\n",
    "        edge_label_color=edge_label_color,\n",
    "        label_pos_offset=label_pos_offset\n",
    "    )\n",
    "    \n",
    "    # Comparative analysis\n",
    "    correlation = analyze_layer_comparison(layer1_results, layer2_results)\n",
    "    \n",
    "    # Detailed statistics\n",
    "    print(\"\\n=== Detailed Statistics ===\")\n",
    "    print(f\"Layer 1 - Original loss: {layer1_results['original_loss']:.6f}\")\n",
    "    print(f\"Layer 2 - Original loss: {layer2_results['original_loss']:.6f}\")\n",
    "    \n",
    "    # Find the most important edges\n",
    "    edge_index = layer1_results['edge_index']\n",
    "    top_k = 3\n",
    "    \n",
    "    for layer_name, results in [(\"Layer 1\", layer1_results), (\"Layer 2\", layer2_results)]:\n",
    "        scores = results['importance_scores']\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        print(f\"\\n{layer_name} most important {top_k} edges:\")\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            source, target = edge_index[0, idx], edge_index[1, idx]\n",
    "            importance = scores[idx]\n",
    "            effect = results['removal_effects'][idx]\n",
    "            print(f\"  {i+1}. Edge {source}-{target}: Importance={importance:.4f}, Effect={effect:.6f}\")\n",
    "    \n",
    "    return layer1_results, layer2_results\n",
    "\n",
    "def visualize_microstructures_with_damage(sample_data, cmap='gray', figsize=(15, 3), \n",
    "                                        border_color='k', border_width=3):\n",
    "    \"\"\"\n",
    "    Visualize microstructures and print DamageStrain values\n",
    "    \"\"\"\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # Get microstructure data and DamageStrain values\n",
    "    micro_features = sample_data.x[:, :-1].view(-1, 21, 21).detach().cpu().numpy()\n",
    "    mfrac_values = sample_data.x[:, -1].detach().cpu().numpy()\n",
    "    damage_strains = sample_data.y.detach().cpu().numpy().flatten()\n",
    "    \n",
    "    # Print DamageStrain values\n",
    "    print(\"\\n=== Node DamageStrain Values ===\")\n",
    "    for i, damage in enumerate(damage_strains):\n",
    "        print(f\"Node {i}: DamageStrain = {damage:.5f}\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 5, figsize=figsize)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        # Draw image\n",
    "        ax.imshow(micro_features[i], cmap=cmap)\n",
    "        \n",
    "        # Create rectangle border\n",
    "        rect = patches.Rectangle((-0.5, -0.5), micro_features[i].shape[1], \n",
    "                                micro_features[i].shape[0], \n",
    "                                linewidth=border_width, edgecolor=border_color, \n",
    "                                facecolor='none', alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add DamageStrain information in title\n",
    "        ax.set_title(f'Node {i}\\nGMF: {mfrac_values[i]:.3f}\\nDamage: {damage_strains[i]:.3f}', \n",
    "                    fontsize=26)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Node Microstructures with DamageStrain Values\", fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Modified main function\n",
    "if __name__ == \"__main__\":\n",
    "    sample_data = train_dataset[1]  # Use sample index 1\n",
    "    \n",
    "    # Perform comprehensive analysis\n",
    "    layer1_results, layer2_results = comprehensive_gcn_analysis(\n",
    "        model, sample_data,\n",
    "        node_size=4000,\n",
    "        label_font_size=32,\n",
    "        edge_width_multiplier=5,\n",
    "        edge_label_size=28,\n",
    "        show_edge_labels=True,\n",
    "        edge_label_color='k',\n",
    "        label_pos_offset=2\n",
    "    )\n",
    "    \n",
    "    # Verify result differences\n",
    "    diff_ratio = np.mean(np.abs(layer1_results['importance_scores'] - layer2_results['importance_scores']))\n",
    "    print(f\"\\nVerification result: Average difference ratio between layer importance = {diff_ratio:.4f}\")\n",
    "    \n",
    "    if diff_ratio < 0.1:\n",
    "        print(\"Warning: Small difference in layer importance, may need to check model structure\")\n",
    "    else:\n",
    "        print(\"Normal: Significant difference in layer importance\")\n",
    "    \n",
    "    # Show microstructure images with DamageStrain values at the end\n",
    "    print(\"\\n=== Displaying microstructure images with DamageStrain values ===\")\n",
    "    visualize_microstructures_with_damage(\n",
    "        sample_data,\n",
    "        cmap='gray_r',\n",
    "        figsize=(20, 4),\n",
    "        border_color='k',\n",
    "        border_width=6\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
